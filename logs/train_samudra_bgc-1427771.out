2025-09-06 23:01:16,809 - INFO - | distributed init (rank 0), gpu 0, world_size 4, dist_url env://
2025-09-06 23:01:17,428 - INFO - | distributed init (rank 3), gpu 3, world_size 4, dist_url env://
2025-09-06 23:01:17,545 - INFO - | distributed init (rank 2), gpu 2, world_size 4, dist_url env://
2025-09-06 23:01:17,555 - INFO - | distributed init (rank 1), gpu 1, world_size 4, dist_url env://
tiger-i04g1:479702:479702 [0] NCCL INFO Bootstrap : Using ib0:10.36.54.40<0>
tiger-i04g1:479702:479702 [0] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
tiger-i04g1:479702:479702 [0] NCCL INFO cudaDriverVersion 13000
NCCL version 2.19.3+cuda12.3
tiger-i04g1:479704:479704 [2] NCCL INFO cudaDriverVersion 13000
tiger-i04g1:479704:479704 [2] NCCL INFO Bootstrap : Using ib0:10.36.54.40<0>
tiger-i04g1:479704:479704 [2] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
tiger-i04g1:479703:479703 [1] NCCL INFO cudaDriverVersion 13000
tiger-i04g1:479703:479703 [1] NCCL INFO Bootstrap : Using ib0:10.36.54.40<0>
tiger-i04g1:479703:479703 [1] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
tiger-i04g1:479705:479705 [3] NCCL INFO cudaDriverVersion 13000
tiger-i04g1:479705:479705 [3] NCCL INFO Bootstrap : Using ib0:10.36.54.40<0>
tiger-i04g1:479705:479705 [3] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
tiger-i04g1:479702:479768 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ib0:10.36.54.40<0>
tiger-i04g1:479702:479768 [0] NCCL INFO Using non-device net plugin version 0
tiger-i04g1:479702:479768 [0] NCCL INFO Using network IB
tiger-i04g1:479704:479769 [2] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ib0:10.36.54.40<0>
tiger-i04g1:479704:479769 [2] NCCL INFO Using non-device net plugin version 0
tiger-i04g1:479704:479769 [2] NCCL INFO Using network IB
tiger-i04g1:479702:479768 [0] NCCL INFO DMA-BUF is available on GPU device 0
tiger-i04g1:479703:479770 [1] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ib0:10.36.54.40<0>
tiger-i04g1:479703:479770 [1] NCCL INFO Using non-device net plugin version 0
tiger-i04g1:479703:479770 [1] NCCL INFO Using network IB
tiger-i04g1:479704:479769 [2] NCCL INFO DMA-BUF is available on GPU device 2
tiger-i04g1:479705:479771 [3] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ib0:10.36.54.40<0>
tiger-i04g1:479705:479771 [3] NCCL INFO Using non-device net plugin version 0
tiger-i04g1:479705:479771 [3] NCCL INFO Using network IB
tiger-i04g1:479703:479770 [1] NCCL INFO DMA-BUF is available on GPU device 1
tiger-i04g1:479705:479771 [3] NCCL INFO DMA-BUF is available on GPU device 3
tiger-i04g1:479705:479771 [3] NCCL INFO comm 0x844d2d0 rank 3 nranks 4 cudaDev 3 nvmlDev 3 busId 45000 commId 0xc53807edb534d5c8 - Init START
tiger-i04g1:479703:479770 [1] NCCL INFO comm 0x9f32d00 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId 18000 commId 0xc53807edb534d5c8 - Init START
tiger-i04g1:479704:479769 [2] NCCL INFO comm 0x8e66780 rank 2 nranks 4 cudaDev 2 nvmlDev 2 busId 3c000 commId 0xc53807edb534d5c8 - Init START
tiger-i04g1:479702:479768 [0] NCCL INFO comm 0x9d165a0 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId a000 commId 0xc53807edb534d5c8 - Init START
tiger-i04g1:479704:479769 [2] NCCL INFO Setting affinity for GPU 2 to 0fffffff
tiger-i04g1:479704:479769 [2] NCCL INFO NVLS multicast support is available on dev 2
tiger-i04g1:479703:479770 [1] NCCL INFO Setting affinity for GPU 1 to 0fffffff
tiger-i04g1:479703:479770 [1] NCCL INFO NVLS multicast support is available on dev 1
tiger-i04g1:479705:479771 [3] NCCL INFO Setting affinity for GPU 3 to 0fffffff
tiger-i04g1:479705:479771 [3] NCCL INFO NVLS multicast support is available on dev 3
tiger-i04g1:479702:479768 [0] NCCL INFO Setting affinity for GPU 0 to 0fffffff
tiger-i04g1:479702:479768 [0] NCCL INFO NVLS multicast support is available on dev 0
tiger-i04g1:479702:479768 [0] NCCL INFO Channel 00/24 :    0   1   2   3
tiger-i04g1:479704:479769 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1 [4] 3/-1/-1->2->1 [5] 3/-1/-1->2->1 [6] 3/-1/-1->2->1 [7] 3/-1/-1->2->1 [8] 3/-1/-1->2->1 [9] 3/-1/-1->2->1 [10] 3/-1/-1->2->1 [11] 3/-1/-1->2->1 [12] 3/-1/-1->2->1 [13] 3/-1/-1->2->1 [14] 3/-1/-1->2->1 [15] 3/-1/-1->2->1 [16] 3/-1/-1->2->1 [17] 3/-1/-1->2->1 [18] 3/-1/-1->2->1 [19] 3/-1/-1->2->1 [20] 3/-1/-1->2->1 [21] 3/-1/-1->2->1 [22] 3/-1/-1->2->1 [23] 3/-1/-1->2->1
tiger-i04g1:479703:479770 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] 2/-1/-1->1->0 [3] 2/-1/-1->1->0 [4] 2/-1/-1->1->0 [5] 2/-1/-1->1->0 [6] 2/-1/-1->1->0 [7] 2/-1/-1->1->0 [8] 2/-1/-1->1->0 [9] 2/-1/-1->1->0 [10] 2/-1/-1->1->0 [11] 2/-1/-1->1->0 [12] 2/-1/-1->1->0 [13] 2/-1/-1->1->0 [14] 2/-1/-1->1->0 [15] 2/-1/-1->1->0 [16] 2/-1/-1->1->0 [17] 2/-1/-1->1->0 [18] 2/-1/-1->1->0 [19] 2/-1/-1->1->0 [20] 2/-1/-1->1->0 [21] 2/-1/-1->1->0 [22] 2/-1/-1->1->0 [23] 2/-1/-1->1->0
tiger-i04g1:479705:479771 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2 [2] -1/-1/-1->3->2 [3] -1/-1/-1->3->2 [4] -1/-1/-1->3->2 [5] -1/-1/-1->3->2 [6] -1/-1/-1->3->2 [7] -1/-1/-1->3->2 [8] -1/-1/-1->3->2 [9] -1/-1/-1->3->2 [10] -1/-1/-1->3->2 [11] -1/-1/-1->3->2 [12] -1/-1/-1->3->2 [13] -1/-1/-1->3->2 [14] -1/-1/-1->3->2 [15] -1/-1/-1->3->2 [16] -1/-1/-1->3->2 [17] -1/-1/-1->3->2 [18] -1/-1/-1->3->2 [19] -1/-1/-1->3->2 [20] -1/-1/-1->3->2 [21] -1/-1/-1->3->2 [22] -1/-1/-1->3->2 [23] -1/-1/-1->3->2
tiger-i04g1:479702:479768 [0] NCCL INFO Channel 01/24 :    0   1   2   3
tiger-i04g1:479704:479769 [2] NCCL INFO P2P Chunksize set to 524288
tiger-i04g1:479703:479770 [1] NCCL INFO P2P Chunksize set to 524288
tiger-i04g1:479705:479771 [3] NCCL INFO P2P Chunksize set to 524288
tiger-i04g1:479702:479768 [0] NCCL INFO Channel 02/24 :    0   1   2   3
tiger-i04g1:479702:479768 [0] NCCL INFO Channel 03/24 :    0   1   2   3
tiger-i04g1:479702:479768 [0] NCCL INFO Channel 04/24 :    0   1   2   3
tiger-i04g1:479702:479768 [0] NCCL INFO Channel 05/24 :    0   1   2   3
tiger-i04g1:479702:479768 [0] NCCL INFO Channel 06/24 :    0   1   2   3
tiger-i04g1:479702:479768 [0] NCCL INFO Channel 07/24 :    0   1   2   3
tiger-i04g1:479702:479768 [0] NCCL INFO Channel 08/24 :    0   1   2   3
tiger-i04g1:479702:479768 [0] NCCL INFO Channel 09/24 :    0   1   2   3
tiger-i04g1:479702:479768 [0] NCCL INFO Channel 10/24 :    0   1   2   3
tiger-i04g1:479702:479768 [0] NCCL INFO Channel 11/24 :    0   1   2   3
tiger-i04g1:479702:479768 [0] NCCL INFO Channel 12/24 :    0   1   2   3
tiger-i04g1:479702:479768 [0] NCCL INFO Channel 13/24 :    0   1   2   3
tiger-i04g1:479702:479768 [0] NCCL INFO Channel 14/24 :    0   1   2   3
tiger-i04g1:479702:479768 [0] NCCL INFO Channel 15/24 :    0   1   2   3
tiger-i04g1:479702:479768 [0] NCCL INFO Channel 16/24 :    0   1   2   3
tiger-i04g1:479702:479768 [0] NCCL INFO Channel 17/24 :    0   1   2   3
tiger-i04g1:479702:479768 [0] NCCL INFO Channel 18/24 :    0   1   2   3
tiger-i04g1:479702:479768 [0] NCCL INFO Channel 19/24 :    0   1   2   3
tiger-i04g1:479702:479768 [0] NCCL INFO Channel 20/24 :    0   1   2   3
tiger-i04g1:479702:479768 [0] NCCL INFO Channel 21/24 :    0   1   2   3
tiger-i04g1:479702:479768 [0] NCCL INFO Channel 22/24 :    0   1   2   3
tiger-i04g1:479702:479768 [0] NCCL INFO Channel 23/24 :    0   1   2   3
tiger-i04g1:479702:479768 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 1/-1/-1->0->-1 [3] 1/-1/-1->0->-1 [4] 1/-1/-1->0->-1 [5] 1/-1/-1->0->-1 [6] 1/-1/-1->0->-1 [7] 1/-1/-1->0->-1 [8] 1/-1/-1->0->-1 [9] 1/-1/-1->0->-1 [10] 1/-1/-1->0->-1 [11] 1/-1/-1->0->-1 [12] 1/-1/-1->0->-1 [13] 1/-1/-1->0->-1 [14] 1/-1/-1->0->-1 [15] 1/-1/-1->0->-1 [16] 1/-1/-1->0->-1 [17] 1/-1/-1->0->-1 [18] 1/-1/-1->0->-1 [19] 1/-1/-1->0->-1 [20] 1/-1/-1->0->-1 [21] 1/-1/-1->0->-1 [22] 1/-1/-1->0->-1 [23] 1/-1/-1->0->-1
tiger-i04g1:479702:479768 [0] NCCL INFO P2P Chunksize set to 524288
tiger-i04g1:479705:479771 [3] NCCL INFO Channel 00/0 : 3[3] -> 0[0] via P2P/CUMEM
tiger-i04g1:479705:479771 [3] NCCL INFO Channel 01/0 : 3[3] -> 0[0] via P2P/CUMEM
tiger-i04g1:479705:479771 [3] NCCL INFO Channel 02/0 : 3[3] -> 0[0] via P2P/CUMEM
tiger-i04g1:479705:479771 [3] NCCL INFO Channel 03/0 : 3[3] -> 0[0] via P2P/CUMEM
tiger-i04g1:479705:479771 [3] NCCL INFO Channel 04/0 : 3[3] -> 0[0] via P2P/CUMEM
tiger-i04g1:479705:479771 [3] NCCL INFO Channel 05/0 : 3[3] -> 0[0] via P2P/CUMEM
tiger-i04g1:479705:479771 [3] NCCL INFO Channel 06/0 : 3[3] -> 0[0] via P2P/CUMEM
tiger-i04g1:479705:479771 [3] NCCL INFO Channel 07/0 : 3[3] -> 0[0] via P2P/CUMEM
tiger-i04g1:479705:479771 [3] NCCL INFO Channel 08/0 : 3[3] -> 0[0] via P2P/CUMEM
tiger-i04g1:479705:479771 [3] NCCL INFO Channel 09/0 : 3[3] -> 0[0] via P2P/CUMEM
tiger-i04g1:479705:479771 [3] NCCL INFO Channel 10/0 : 3[3] -> 0[0] via P2P/CUMEM
tiger-i04g1:479705:479771 [3] NCCL INFO Channel 11/0 : 3[3] -> 0[0] via P2P/CUMEM
tiger-i04g1:479705:479771 [3] NCCL INFO Channel 12/0 : 3[3] -> 0[0] via P2P/CUMEM
tiger-i04g1:479705:479771 [3] NCCL INFO Channel 13/0 : 3[3] -> 0[0] via P2P/CUMEM
tiger-i04g1:479705:479771 [3] NCCL INFO Channel 14/0 : 3[3] -> 0[0] via P2P/CUMEM
tiger-i04g1:479705:479771 [3] NCCL INFO Channel 15/0 : 3[3] -> 0[0] via P2P/CUMEM
tiger-i04g1:479705:479771 [3] NCCL INFO Channel 16/0 : 3[3] -> 0[0] via P2P/CUMEM
tiger-i04g1:479705:479771 [3] NCCL INFO Channel 17/0 : 3[3] -> 0[0] via P2P/CUMEM
tiger-i04g1:479705:479771 [3] NCCL INFO Channel 18/0 : 3[3] -> 0[0] via P2P/CUMEM
tiger-i04g1:479705:479771 [3] NCCL INFO Channel 19/0 : 3[3] -> 0[0] via P2P/CUMEM
tiger-i04g1:479704:479769 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/CUMEM
tiger-i04g1:479705:479771 [3] NCCL INFO Channel 20/0 : 3[3] -> 0[0] via P2P/CUMEM
tiger-i04g1:479704:479769 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[3] via P2P/CUMEM
tiger-i04g1:479704:479769 [2] NCCL INFO Channel 02/0 : 2[2] -> 3[3] via P2P/CUMEM
tiger-i04g1:479704:479769 [2] NCCL INFO Channel 03/0 : 2[2] -> 3[3] via P2P/CUMEM
tiger-i04g1:479704:479769 [2] NCCL INFO Channel 04/0 : 2[2] -> 3[3] via P2P/CUMEM
tiger-i04g1:479704:479769 [2] NCCL INFO Channel 05/0 : 2[2] -> 3[3] via P2P/CUMEM
tiger-i04g1:479704:479769 [2] NCCL INFO Channel 06/0 : 2[2] -> 3[3] via P2P/CUMEM
tiger-i04g1:479704:479769 [2] NCCL INFO Channel 07/0 : 2[2] -> 3[3] via P2P/CUMEM
tiger-i04g1:479704:479769 [2] NCCL INFO Channel 08/0 : 2[2] -> 3[3] via P2P/CUMEM
tiger-i04g1:479704:479769 [2] NCCL INFO Channel 09/0 : 2[2] -> 3[3] via P2P/CUMEM
tiger-i04g1:479704:479769 [2] NCCL INFO Channel 10/0 : 2[2] -> 3[3] via P2P/CUMEM
tiger-i04g1:479704:479769 [2] NCCL INFO Channel 11/0 : 2[2] -> 3[3] via P2P/CUMEM
tiger-i04g1:479704:479769 [2] NCCL INFO Channel 12/0 : 2[2] -> 3[3] via P2P/CUMEM
tiger-i04g1:479704:479769 [2] NCCL INFO Channel 13/0 : 2[2] -> 3[3] via P2P/CUMEM
tiger-i04g1:479704:479769 [2] NCCL INFO Channel 14/0 : 2[2] -> 3[3] via P2P/CUMEM
tiger-i04g1:479704:479769 [2] NCCL INFO Channel 15/0 : 2[2] -> 3[3] via P2P/CUMEM
tiger-i04g1:479704:479769 [2] NCCL INFO Channel 16/0 : 2[2] -> 3[3] via P2P/CUMEM
tiger-i04g1:479704:479769 [2] NCCL INFO Channel 17/0 : 2[2] -> 3[3] via P2P/CUMEM
tiger-i04g1:479704:479769 [2] NCCL INFO Channel 18/0 : 2[2] -> 3[3] via P2P/CUMEM
tiger-i04g1:479704:479769 [2] NCCL INFO Channel 19/0 : 2[2] -> 3[3] via P2P/CUMEM
tiger-i04g1:479704:479769 [2] NCCL INFO Channel 20/0 : 2[2] -> 3[3] via P2P/CUMEM
tiger-i04g1:479704:479769 [2] NCCL INFO Channel 21/0 : 2[2] -> 3[3] via P2P/CUMEM
tiger-i04g1:479704:479769 [2] NCCL INFO Channel 22/0 : 2[2] -> 3[3] via P2P/CUMEM
tiger-i04g1:479704:479769 [2] NCCL INFO Channel 23/0 : 2[2] -> 3[3] via P2P/CUMEM
tiger-i04g1:479703:479770 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[2] via P2P/CUMEM
tiger-i04g1:479705:479771 [3] NCCL INFO Channel 21/0 : 3[3] -> 0[0] via P2P/CUMEM
tiger-i04g1:479703:479770 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[2] via P2P/CUMEM
tiger-i04g1:479705:479771 [3] NCCL INFO Channel 22/0 : 3[3] -> 0[0] via P2P/CUMEM
tiger-i04g1:479703:479770 [1] NCCL INFO Channel 02/0 : 1[1] -> 2[2] via P2P/CUMEM
tiger-i04g1:479703:479770 [1] NCCL INFO Channel 03/0 : 1[1] -> 2[2] via P2P/CUMEM
tiger-i04g1:479703:479770 [1] NCCL INFO Channel 04/0 : 1[1] -> 2[2] via P2P/CUMEM
tiger-i04g1:479703:479770 [1] NCCL INFO Channel 05/0 : 1[1] -> 2[2] via P2P/CUMEM
tiger-i04g1:479703:479770 [1] NCCL INFO Channel 06/0 : 1[1] -> 2[2] via P2P/CUMEM
tiger-i04g1:479703:479770 [1] NCCL INFO Channel 07/0 : 1[1] -> 2[2] via P2P/CUMEM
tiger-i04g1:479703:479770 [1] NCCL INFO Channel 08/0 : 1[1] -> 2[2] via P2P/CUMEM
tiger-i04g1:479703:479770 [1] NCCL INFO Channel 09/0 : 1[1] -> 2[2] via P2P/CUMEM
tiger-i04g1:479703:479770 [1] NCCL INFO Channel 10/0 : 1[1] -> 2[2] via P2P/CUMEM
tiger-i04g1:479703:479770 [1] NCCL INFO Channel 11/0 : 1[1] -> 2[2] via P2P/CUMEM
tiger-i04g1:479703:479770 [1] NCCL INFO Channel 12/0 : 1[1] -> 2[2] via P2P/CUMEM
tiger-i04g1:479703:479770 [1] NCCL INFO Channel 13/0 : 1[1] -> 2[2] via P2P/CUMEM
tiger-i04g1:479703:479770 [1] NCCL INFO Channel 14/0 : 1[1] -> 2[2] via P2P/CUMEM
tiger-i04g1:479703:479770 [1] NCCL INFO Channel 15/0 : 1[1] -> 2[2] via P2P/CUMEM
tiger-i04g1:479703:479770 [1] NCCL INFO Channel 16/0 : 1[1] -> 2[2] via P2P/CUMEM
tiger-i04g1:479703:479770 [1] NCCL INFO Channel 17/0 : 1[1] -> 2[2] via P2P/CUMEM
tiger-i04g1:479703:479770 [1] NCCL INFO Channel 18/0 : 1[1] -> 2[2] via P2P/CUMEM
tiger-i04g1:479703:479770 [1] NCCL INFO Channel 19/0 : 1[1] -> 2[2] via P2P/CUMEM
tiger-i04g1:479703:479770 [1] NCCL INFO Channel 20/0 : 1[1] -> 2[2] via P2P/CUMEM
tiger-i04g1:479703:479770 [1] NCCL INFO Channel 21/0 : 1[1] -> 2[2] via P2P/CUMEM
tiger-i04g1:479703:479770 [1] NCCL INFO Channel 22/0 : 1[1] -> 2[2] via P2P/CUMEM
tiger-i04g1:479703:479770 [1] NCCL INFO Channel 23/0 : 1[1] -> 2[2] via P2P/CUMEM
tiger-i04g1:479705:479771 [3] NCCL INFO Channel 23/0 : 3[3] -> 0[0] via P2P/CUMEM
tiger-i04g1:479702:479768 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/CUMEM
tiger-i04g1:479702:479768 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/CUMEM
tiger-i04g1:479702:479768 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[1] via P2P/CUMEM
tiger-i04g1:479702:479768 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[1] via P2P/CUMEM
tiger-i04g1:479702:479768 [0] NCCL INFO Channel 04/0 : 0[0] -> 1[1] via P2P/CUMEM
tiger-i04g1:479702:479768 [0] NCCL INFO Channel 05/0 : 0[0] -> 1[1] via P2P/CUMEM
tiger-i04g1:479702:479768 [0] NCCL INFO Channel 06/0 : 0[0] -> 1[1] via P2P/CUMEM
tiger-i04g1:479702:479768 [0] NCCL INFO Channel 07/0 : 0[0] -> 1[1] via P2P/CUMEM
tiger-i04g1:479702:479768 [0] NCCL INFO Channel 08/0 : 0[0] -> 1[1] via P2P/CUMEM
tiger-i04g1:479702:479768 [0] NCCL INFO Channel 09/0 : 0[0] -> 1[1] via P2P/CUMEM
tiger-i04g1:479702:479768 [0] NCCL INFO Channel 10/0 : 0[0] -> 1[1] via P2P/CUMEM
tiger-i04g1:479702:479768 [0] NCCL INFO Channel 11/0 : 0[0] -> 1[1] via P2P/CUMEM
tiger-i04g1:479702:479768 [0] NCCL INFO Channel 12/0 : 0[0] -> 1[1] via P2P/CUMEM
tiger-i04g1:479702:479768 [0] NCCL INFO Channel 13/0 : 0[0] -> 1[1] via P2P/CUMEM
tiger-i04g1:479702:479768 [0] NCCL INFO Channel 14/0 : 0[0] -> 1[1] via P2P/CUMEM
tiger-i04g1:479702:479768 [0] NCCL INFO Channel 15/0 : 0[0] -> 1[1] via P2P/CUMEM
tiger-i04g1:479702:479768 [0] NCCL INFO Channel 16/0 : 0[0] -> 1[1] via P2P/CUMEM
tiger-i04g1:479702:479768 [0] NCCL INFO Channel 17/0 : 0[0] -> 1[1] via P2P/CUMEM
tiger-i04g1:479702:479768 [0] NCCL INFO Channel 18/0 : 0[0] -> 1[1] via P2P/CUMEM
tiger-i04g1:479702:479768 [0] NCCL INFO Channel 19/0 : 0[0] -> 1[1] via P2P/CUMEM
tiger-i04g1:479702:479768 [0] NCCL INFO Channel 20/0 : 0[0] -> 1[1] via P2P/CUMEM
tiger-i04g1:479702:479768 [0] NCCL INFO Channel 21/0 : 0[0] -> 1[1] via P2P/CUMEM
tiger-i04g1:479702:479768 [0] NCCL INFO Channel 22/0 : 0[0] -> 1[1] via P2P/CUMEM
tiger-i04g1:479702:479768 [0] NCCL INFO Channel 23/0 : 0[0] -> 1[1] via P2P/CUMEM
tiger-i04g1:479704:479769 [2] NCCL INFO Connected all rings
tiger-i04g1:479705:479771 [3] NCCL INFO Connected all rings
tiger-i04g1:479702:479768 [0] NCCL INFO Connected all rings
tiger-i04g1:479705:479771 [3] NCCL INFO Channel 00/0 : 3[3] -> 2[2] via P2P/CUMEM
tiger-i04g1:479703:479770 [1] NCCL INFO Connected all rings
tiger-i04g1:479705:479771 [3] NCCL INFO Channel 01/0 : 3[3] -> 2[2] via P2P/CUMEM
tiger-i04g1:479705:479771 [3] NCCL INFO Channel 02/0 : 3[3] -> 2[2] via P2P/CUMEM
tiger-i04g1:479705:479771 [3] NCCL INFO Channel 03/0 : 3[3] -> 2[2] via P2P/CUMEM
tiger-i04g1:479705:479771 [3] NCCL INFO Channel 04/0 : 3[3] -> 2[2] via P2P/CUMEM
tiger-i04g1:479705:479771 [3] NCCL INFO Channel 05/0 : 3[3] -> 2[2] via P2P/CUMEM
tiger-i04g1:479705:479771 [3] NCCL INFO Channel 06/0 : 3[3] -> 2[2] via P2P/CUMEM
tiger-i04g1:479705:479771 [3] NCCL INFO Channel 07/0 : 3[3] -> 2[2] via P2P/CUMEM
tiger-i04g1:479704:479769 [2] NCCL INFO Channel 00/0 : 2[2] -> 1[1] via P2P/CUMEM
tiger-i04g1:479704:479769 [2] NCCL INFO Channel 01/0 : 2[2] -> 1[1] via P2P/CUMEM
tiger-i04g1:479704:479769 [2] NCCL INFO Channel 02/0 : 2[2] -> 1[1] via P2P/CUMEM
tiger-i04g1:479704:479769 [2] NCCL INFO Channel 03/0 : 2[2] -> 1[1] via P2P/CUMEM
tiger-i04g1:479704:479769 [2] NCCL INFO Channel 04/0 : 2[2] -> 1[1] via P2P/CUMEM
tiger-i04g1:479704:479769 [2] NCCL INFO Channel 05/0 : 2[2] -> 1[1] via P2P/CUMEM
tiger-i04g1:479704:479769 [2] NCCL INFO Channel 06/0 : 2[2] -> 1[1] via P2P/CUMEM
tiger-i04g1:479704:479769 [2] NCCL INFO Channel 07/0 : 2[2] -> 1[1] via P2P/CUMEM
tiger-i04g1:479704:479769 [2] NCCL INFO Channel 08/0 : 2[2] -> 1[1] via P2P/CUMEM
tiger-i04g1:479704:479769 [2] NCCL INFO Channel 09/0 : 2[2] -> 1[1] via P2P/CUMEM
tiger-i04g1:479705:479771 [3] NCCL INFO Channel 08/0 : 3[3] -> 2[2] via P2P/CUMEM
tiger-i04g1:479704:479769 [2] NCCL INFO Channel 10/0 : 2[2] -> 1[1] via P2P/CUMEM
tiger-i04g1:479705:479771 [3] NCCL INFO Channel 09/0 : 3[3] -> 2[2] via P2P/CUMEM
tiger-i04g1:479705:479771 [3] NCCL INFO Channel 10/0 : 3[3] -> 2[2] via P2P/CUMEM
tiger-i04g1:479705:479771 [3] NCCL INFO Channel 11/0 : 3[3] -> 2[2] via P2P/CUMEM
tiger-i04g1:479705:479771 [3] NCCL INFO Channel 12/0 : 3[3] -> 2[2] via P2P/CUMEM
tiger-i04g1:479704:479769 [2] NCCL INFO Channel 11/0 : 2[2] -> 1[1] via P2P/CUMEM
tiger-i04g1:479705:479771 [3] NCCL INFO Channel 13/0 : 3[3] -> 2[2] via P2P/CUMEM
tiger-i04g1:479705:479771 [3] NCCL INFO Channel 14/0 : 3[3] -> 2[2] via P2P/CUMEM
tiger-i04g1:479705:479771 [3] NCCL INFO Channel 15/0 : 3[3] -> 2[2] via P2P/CUMEM
tiger-i04g1:479705:479771 [3] NCCL INFO Channel 16/0 : 3[3] -> 2[2] via P2P/CUMEM
tiger-i04g1:479705:479771 [3] NCCL INFO Channel 17/0 : 3[3] -> 2[2] via P2P/CUMEM
tiger-i04g1:479705:479771 [3] NCCL INFO Channel 18/0 : 3[3] -> 2[2] via P2P/CUMEM
tiger-i04g1:479705:479771 [3] NCCL INFO Channel 19/0 : 3[3] -> 2[2] via P2P/CUMEM
tiger-i04g1:479705:479771 [3] NCCL INFO Channel 20/0 : 3[3] -> 2[2] via P2P/CUMEM
tiger-i04g1:479705:479771 [3] NCCL INFO Channel 21/0 : 3[3] -> 2[2] via P2P/CUMEM
tiger-i04g1:479705:479771 [3] NCCL INFO Channel 22/0 : 3[3] -> 2[2] via P2P/CUMEM
tiger-i04g1:479705:479771 [3] NCCL INFO Channel 23/0 : 3[3] -> 2[2] via P2P/CUMEM
tiger-i04g1:479704:479769 [2] NCCL INFO Channel 12/0 : 2[2] -> 1[1] via P2P/CUMEM
tiger-i04g1:479704:479769 [2] NCCL INFO Channel 13/0 : 2[2] -> 1[1] via P2P/CUMEM
tiger-i04g1:479704:479769 [2] NCCL INFO Channel 14/0 : 2[2] -> 1[1] via P2P/CUMEM
tiger-i04g1:479704:479769 [2] NCCL INFO Channel 15/0 : 2[2] -> 1[1] via P2P/CUMEM
tiger-i04g1:479703:479770 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/CUMEM
tiger-i04g1:479703:479770 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/CUMEM
tiger-i04g1:479703:479770 [1] NCCL INFO Channel 02/0 : 1[1] -> 0[0] via P2P/CUMEM
tiger-i04g1:479703:479770 [1] NCCL INFO Channel 03/0 : 1[1] -> 0[0] via P2P/CUMEM
tiger-i04g1:479703:479770 [1] NCCL INFO Channel 04/0 : 1[1] -> 0[0] via P2P/CUMEM
tiger-i04g1:479703:479770 [1] NCCL INFO Channel 05/0 : 1[1] -> 0[0] via P2P/CUMEM
tiger-i04g1:479703:479770 [1] NCCL INFO Channel 06/0 : 1[1] -> 0[0] via P2P/CUMEM
tiger-i04g1:479703:479770 [1] NCCL INFO Channel 07/0 : 1[1] -> 0[0] via P2P/CUMEM
tiger-i04g1:479703:479770 [1] NCCL INFO Channel 08/0 : 1[1] -> 0[0] via P2P/CUMEM
tiger-i04g1:479703:479770 [1] NCCL INFO Channel 09/0 : 1[1] -> 0[0] via P2P/CUMEM
tiger-i04g1:479703:479770 [1] NCCL INFO Channel 10/0 : 1[1] -> 0[0] via P2P/CUMEM
tiger-i04g1:479703:479770 [1] NCCL INFO Channel 11/0 : 1[1] -> 0[0] via P2P/CUMEM
tiger-i04g1:479703:479770 [1] NCCL INFO Channel 12/0 : 1[1] -> 0[0] via P2P/CUMEM
tiger-i04g1:479703:479770 [1] NCCL INFO Channel 13/0 : 1[1] -> 0[0] via P2P/CUMEM
tiger-i04g1:479703:479770 [1] NCCL INFO Channel 14/0 : 1[1] -> 0[0] via P2P/CUMEM
tiger-i04g1:479704:479769 [2] NCCL INFO Channel 16/0 : 2[2] -> 1[1] via P2P/CUMEM
tiger-i04g1:479704:479769 [2] NCCL INFO Channel 17/0 : 2[2] -> 1[1] via P2P/CUMEM
tiger-i04g1:479703:479770 [1] NCCL INFO Channel 15/0 : 1[1] -> 0[0] via P2P/CUMEM
tiger-i04g1:479703:479770 [1] NCCL INFO Channel 16/0 : 1[1] -> 0[0] via P2P/CUMEM
tiger-i04g1:479704:479769 [2] NCCL INFO Channel 18/0 : 2[2] -> 1[1] via P2P/CUMEM
tiger-i04g1:479704:479769 [2] NCCL INFO Channel 19/0 : 2[2] -> 1[1] via P2P/CUMEM
tiger-i04g1:479704:479769 [2] NCCL INFO Channel 20/0 : 2[2] -> 1[1] via P2P/CUMEM
tiger-i04g1:479704:479769 [2] NCCL INFO Channel 21/0 : 2[2] -> 1[1] via P2P/CUMEM
tiger-i04g1:479704:479769 [2] NCCL INFO Channel 22/0 : 2[2] -> 1[1] via P2P/CUMEM
tiger-i04g1:479704:479769 [2] NCCL INFO Channel 23/0 : 2[2] -> 1[1] via P2P/CUMEM
tiger-i04g1:479703:479770 [1] NCCL INFO Channel 17/0 : 1[1] -> 0[0] via P2P/CUMEM
tiger-i04g1:479703:479770 [1] NCCL INFO Channel 18/0 : 1[1] -> 0[0] via P2P/CUMEM
tiger-i04g1:479703:479770 [1] NCCL INFO Channel 19/0 : 1[1] -> 0[0] via P2P/CUMEM
tiger-i04g1:479703:479770 [1] NCCL INFO Channel 20/0 : 1[1] -> 0[0] via P2P/CUMEM
tiger-i04g1:479703:479770 [1] NCCL INFO Channel 21/0 : 1[1] -> 0[0] via P2P/CUMEM
tiger-i04g1:479703:479770 [1] NCCL INFO Channel 22/0 : 1[1] -> 0[0] via P2P/CUMEM
tiger-i04g1:479703:479770 [1] NCCL INFO Channel 23/0 : 1[1] -> 0[0] via P2P/CUMEM
tiger-i04g1:479702:479768 [0] NCCL INFO Connected all trees
tiger-i04g1:479703:479770 [1] NCCL INFO Connected all trees
tiger-i04g1:479704:479769 [2] NCCL INFO Connected all trees
tiger-i04g1:479705:479771 [3] NCCL INFO Connected all trees
tiger-i04g1:479705:479771 [3] NCCL INFO NVLS comm 0x844d2d0 headRank 3 nHeads 4 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 805306368
tiger-i04g1:479702:479768 [0] NCCL INFO NVLS comm 0x9d165a0 headRank 0 nHeads 4 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 805306368
tiger-i04g1:479703:479770 [1] NCCL INFO NVLS comm 0x9f32d00 headRank 1 nHeads 4 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 805306368
tiger-i04g1:479704:479769 [2] NCCL INFO NVLS comm 0x8e66780 headRank 2 nHeads 4 buffSize 4194304 memSize 2097152 nvlsPerRankSize 201326592 nvlsTotalSize 805306368
tiger-i04g1:479702:479768 [0] NCCL INFO Connected NVLS tree
tiger-i04g1:479702:479768 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
tiger-i04g1:479702:479768 [0] NCCL INFO 24 coll channels, 16 nvls channels, 32 p2p channels, 32 p2p channels per peer
tiger-i04g1:479705:479771 [3] NCCL INFO Connected NVLS tree
tiger-i04g1:479705:479771 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
tiger-i04g1:479705:479771 [3] NCCL INFO 24 coll channels, 16 nvls channels, 32 p2p channels, 32 p2p channels per peer
tiger-i04g1:479703:479770 [1] NCCL INFO Connected NVLS tree
tiger-i04g1:479703:479770 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
tiger-i04g1:479703:479770 [1] NCCL INFO 24 coll channels, 16 nvls channels, 32 p2p channels, 32 p2p channels per peer
tiger-i04g1:479704:479769 [2] NCCL INFO Connected NVLS tree
tiger-i04g1:479704:479769 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
tiger-i04g1:479704:479769 [2] NCCL INFO 24 coll channels, 16 nvls channels, 32 p2p channels, 32 p2p channels per peer
tiger-i04g1:479705:479771 [3] NCCL INFO comm 0x844d2d0 rank 3 nranks 4 cudaDev 3 nvmlDev 3 busId 45000 commId 0xc53807edb534d5c8 - Init COMPLETE
tiger-i04g1:479703:479770 [1] NCCL INFO comm 0x9f32d00 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId 18000 commId 0xc53807edb534d5c8 - Init COMPLETE
tiger-i04g1:479702:479768 [0] NCCL INFO comm 0x9d165a0 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId a000 commId 0xc53807edb534d5c8 - Init COMPLETE
tiger-i04g1:479704:479769 [2] NCCL INFO comm 0x8e66780 rank 2 nranks 4 cudaDev 2 nvmlDev 2 busId 3c000 commId 0xc53807edb534d5c8 - Init COMPLETE
2025-09-06 23:01:21,489 - INFO - Prognostic variables: CT_0, CT_1, CT_2, CT_3, CT_4, CT_5, CT_6, CT_7, CT_8, CT_9, CT_10, CT_11, CT_12, CT_13, CT_14, CT_15, CT_16, CT_17, CT_18, CT_19, CT_20, CT_21, CT_22, CT_23, CT_24, CT_25, CT_26, CT_27, CT_28, CT_29, CT_30, CT_31, CT_32, CT_33, CT_34, CT_35, CT_36, CT_37, CT_38, CT_39, CT_40, CT_41, CT_42, CT_43, SA_0, SA_1, SA_2, SA_3, SA_4, SA_5, SA_6, SA_7, SA_8, SA_9, SA_10, SA_11, SA_12, SA_13, SA_14, SA_15, SA_16, SA_17, SA_18, SA_19, SA_20, SA_21, SA_22, SA_23, SA_24, SA_25, SA_26, SA_27, SA_28, SA_29, SA_30, SA_31, SA_32, SA_33, SA_34, SA_35, SA_36, SA_37, SA_38, SA_39, SA_40, SA_41, SA_42, SA_43, o2_0, o2_1, o2_2, o2_3, o2_4, o2_5, o2_6, o2_7, o2_8, o2_9, o2_10, o2_11, o2_12, o2_13, o2_14, o2_15, o2_16, o2_17, o2_18, o2_19, o2_20, o2_21, o2_22, o2_23, o2_24, o2_25, o2_26, o2_27, o2_28, o2_29, o2_30, o2_31, o2_32, o2_33, o2_34, o2_35, o2_36, o2_37, o2_38, o2_39, o2_40, o2_41, o2_42, o2_43, dic_0, dic_1, dic_2, dic_3, dic_4, dic_5, dic_6, dic_7, dic_8, dic_9, dic_10, dic_11, dic_12, dic_13, dic_14, dic_15, dic_16, dic_17, dic_18, dic_19, dic_20, dic_21, dic_22, dic_23, dic_24, dic_25, dic_26, dic_27, dic_28, dic_29, dic_30, dic_31, dic_32, dic_33, dic_34, dic_35, dic_36, dic_37, dic_38, dic_39, dic_40, dic_41, dic_42, dic_43
2025-09-06 23:01:21,490 - INFO - Boundary variables: tauuo, tauvo, Qnet
2025-09-06 23:01:21,490 - INFO - Levels: 44
2025-09-06 23:01:21,495 - INFO - Number of inputs: (hist + 1) * prognostic_vars + boundary_vars = 355
2025-09-06 23:01:21,495 - INFO - Number of outputs: (hist + 1) * prognostic_vars = 352
2025-09-06 23:01:21,495 - INFO - Loading data
2025-09-06 23:01:24,100 - INFO - Instantiating model samudra_bgc
[23:01:24.116217] SamudraBGC init - Wet mask shape: torch.Size([352, 270, 180])
[23:01:24.116266] Expected spatial dims based on data: y=270, x=180
2025-09-06 23:01:27,413 - INFO - Loss = mse
2025-09-06 23:01:28,568 - INFO - Inference will use input at time 2024-01-01T00:00:00.000000000 and produce output at 2024-01-03T00:00:00.000000000
2025-09-06 23:01:28,595 - INFO - Starting training at step 4
2025-09-06 23:01:32,722 - INFO - Instantiating torch loaders
[23:08:49.423721] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:08:49.447850] Wet mask shape: torch.Size([352, 270, 180])
[23:08:50.502290] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:08:50.502362] Wet mask shape: torch.Size([352, 270, 180])
[23:08:51.145936] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:08:51.146017] Wet mask shape: torch.Size([352, 270, 180])
[23:08:51.683142] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:08:51.683208] Wet mask shape: torch.Size([352, 270, 180])
2025-09-06 23:09:10,967 - INFO - Training Epoch: [0]  [  0/318]  eta: 1 day, 16:28:41  lr: 0.000200  loss: 4.9349 (4.9349)  time: 458.243(458.243)  data: 378.638(378.638)  max cpu mem: 4314  max gpu mem: 136476
[23:09:13.658674] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:09:13.658761] Wet mask shape: torch.Size([352, 270, 180])
[23:09:14.049357] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:09:14.049435] Wet mask shape: torch.Size([352, 270, 180])
[23:09:14.578692] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:09:14.578766] Wet mask shape: torch.Size([352, 270, 180])
[23:09:15.438106] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:09:15.438176] Wet mask shape: torch.Size([352, 270, 180])
2025-09-06 23:09:16,579 - INFO - Training Epoch: [0]  [  1/318]  eta: 20:25:17  lr: 0.000200  loss: 4.9349 (6.1823)  time: 5.593(5.593)  data: 0.000(0.000)  max cpu mem: 4314  max gpu mem: 136476
[23:09:18.719349] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:09:18.722653] Wet mask shape: torch.Size([352, 270, 180])
[23:09:19.080278] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:09:19.080354] Wet mask shape: torch.Size([352, 270, 180])
[23:09:19.543238] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:09:19.543307] Wet mask shape: torch.Size([352, 270, 180])
[23:09:19.909566] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:09:19.909638] Wet mask shape: torch.Size([352, 270, 180])
2025-09-06 23:09:21,062 - INFO - Training Epoch: [0]  [  2/318]  eta: 13:42:09  lr: 0.000200  loss: 7.4296 (12.2413)  time: 4.482(4.482)  data: 0.000(0.000)  max cpu mem: 4314  max gpu mem: 136476
[23:09:24.101373] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:09:24.101444] Wet mask shape: torch.Size([352, 270, 180])
[23:09:24.598309] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:09:24.598372] Wet mask shape: torch.Size([352, 270, 180])
[23:09:25.106139] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:09:25.106206] Wet mask shape: torch.Size([352, 270, 180])
[23:09:25.617399] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:09:25.617468] Wet mask shape: torch.Size([352, 270, 180])
2025-09-06 23:09:26,757 - INFO - Training Epoch: [0]  [  3/318]  eta: 10:22:07  lr: 0.000200  loss: 7.4296 (11.2138)  time: 5.681(5.681)  data: 0.000(0.000)  max cpu mem: 4314  max gpu mem: 136476
[23:09:29.820670] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:09:29.820741] Wet mask shape: torch.Size([352, 270, 180])
[23:09:30.284274] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:09:30.284340] Wet mask shape: torch.Size([352, 270, 180])
[23:09:30.742676] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:09:30.742746] Wet mask shape: torch.Size([352, 270, 180])
[23:09:31.157208] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:09:31.157284] Wet mask shape: torch.Size([352, 270, 180])
2025-09-06 23:09:32,312 - INFO - Training Epoch: [0]  [  4/318]  eta: 8:21:55  lr: 0.000200  loss: 7.4296 (10.0185)  time: 5.554(5.554)  data: 0.000(0.000)  max cpu mem: 4314  max gpu mem: 136476
[23:09:32.810832] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:09:32.966263] Wet mask shape: torch.Size([352, 270, 180])
[23:09:33.270719] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:09:33.307146] Wet mask shape: torch.Size([352, 270, 180])
[23:09:33.439349] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:09:33.439401] Wet mask shape: torch.Size([352, 270, 180])
[23:09:33.596711] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:09:33.596779] Wet mask shape: torch.Size([352, 270, 180])
2025-09-06 23:09:34,721 - INFO - Training Epoch: [0]  [  5/318]  eta: 6:59:02  lr: 0.000200  loss: 5.2374 (8.8274)  time: 2.408(2.408)  data: 0.000(0.000)  max cpu mem: 4314  max gpu mem: 136476
[23:09:36.346973] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:09:36.347042] Wet mask shape: torch.Size([352, 270, 180])
[23:09:36.735315] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:09:36.735383] Wet mask shape: torch.Size([352, 270, 180])
[23:09:37.068452] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:09:37.068543] Wet mask shape: torch.Size([352, 270, 180])
[23:09:37.312367] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:09:37.312434] Wet mask shape: torch.Size([352, 270, 180])
2025-09-06 23:09:38,421 - INFO - Training Epoch: [0]  [  6/318]  eta: 6:00:46  lr: 0.000200  loss: 5.2374 (7.9273)  time: 3.700(3.700)  data: 0.000(0.000)  max cpu mem: 4314  max gpu mem: 136476
[23:09:39.853393] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:09:39.853462] Wet mask shape: torch.Size([352, 270, 180])
[23:09:40.160370] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:09:40.160434] Wet mask shape: torch.Size([352, 270, 180])
[23:09:40.479747] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:09:40.479815] Wet mask shape: torch.Size([352, 270, 180])
[23:09:40.641193] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:09:40.641258] Wet mask shape: torch.Size([352, 270, 180])
2025-09-06 23:09:41,783 - INFO - Training Epoch: [0]  [  7/318]  eta: 5:16:50  lr: 0.000200  loss: 4.9349 (7.2624)  time: 3.361(3.361)  data: 0.000(0.000)  max cpu mem: 4314  max gpu mem: 136476
[23:09:44.143413] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:09:44.156598] Wet mask shape: torch.Size([352, 270, 180])
[23:09:44.654484] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:09:44.672618] Wet mask shape: torch.Size([352, 270, 180])
[23:09:45.104344] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:09:45.104408] Wet mask shape: torch.Size([352, 270, 180])
[23:09:45.620734] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:09:45.620801] Wet mask shape: torch.Size([352, 270, 180])
2025-09-06 23:09:46,781 - INFO - Training Epoch: [0]  [  8/318]  eta: 4:43:36  lr: 0.000200  loss: 4.9349 (6.6016)  time: 4.998(4.998)  data: 0.004(0.004)  max cpu mem: 4314  max gpu mem: 136476
[23:09:48.837786] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:09:48.837863] Wet mask shape: torch.Size([352, 270, 180])
[23:09:49.195349] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:09:49.195419] Wet mask shape: torch.Size([352, 270, 180])
[23:09:49.518337] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:09:49.518410] Wet mask shape: torch.Size([352, 270, 180])
[23:09:49.920323] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:09:49.920391] Wet mask shape: torch.Size([352, 270, 180])
2025-09-06 23:09:51,041 - INFO - Training Epoch: [0]  [  9/318]  eta: 4:16:36  lr: 0.000200  loss: 2.8716 (6.0588)  time: 4.259(4.259)  data: 0.000(0.000)  max cpu mem: 4314  max gpu mem: 136476
[23:09:52.529337] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:09:52.529410] Wet mask shape: torch.Size([352, 270, 180])
[23:09:52.864110] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:09:52.864178] Wet mask shape: torch.Size([352, 270, 180])
[23:09:53.208425] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:09:53.208504] Wet mask shape: torch.Size([352, 270, 180])
[23:09:53.538449] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:09:53.538550] Wet mask shape: torch.Size([352, 270, 180])
2025-09-06 23:09:54,683 - INFO - Training Epoch: [0]  [ 10/318]  eta: 3:54:13  lr: 0.000200  loss: 2.8716 (5.6098)  time: 3.640(3.640)  data: 0.000(0.000)  max cpu mem: 4314  max gpu mem: 136476
[23:09:56.163338] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:09:56.163411] Wet mask shape: torch.Size([352, 270, 180])
[23:09:56.505972] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:09:56.506044] Wet mask shape: torch.Size([352, 270, 180])
[23:09:56.896732] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:09:56.896802] Wet mask shape: torch.Size([352, 270, 180])
[23:09:57.295599] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:09:57.295674] Wet mask shape: torch.Size([352, 270, 180])
2025-09-06 23:09:58,420 - INFO - Training Epoch: [0]  [ 11/318]  eta: 3:35:36  lr: 0.000200  loss: 2.6079 (5.2323)  time: 3.736(3.736)  data: 0.000(0.000)  max cpu mem: 4314  max gpu mem: 136476
[23:10:00.027200] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:10:00.027283] Wet mask shape: torch.Size([352, 270, 180])
[23:10:00.433508] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:10:00.437572] Wet mask shape: torch.Size([352, 270, 180])
[23:10:00.897416] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:10:00.897494] Wet mask shape: torch.Size([352, 270, 180])
[23:10:01.293357] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:10:01.293438] Wet mask shape: torch.Size([352, 270, 180])
2025-09-06 23:10:02,444 - INFO - Training Epoch: [0]  [ 12/318]  eta: 3:19:56  lr: 0.000200  loss: 2.6079 (4.9028)  time: 4.021(4.021)  data: 0.000(0.000)  max cpu mem: 4314  max gpu mem: 136476
[23:10:03.870442] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:10:03.870523] Wet mask shape: torch.Size([352, 270, 180])
[23:10:04.288692] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:10:04.288764] Wet mask shape: torch.Size([352, 270, 180])
[23:10:04.704099] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:10:04.704171] Wet mask shape: torch.Size([352, 270, 180])
[23:10:05.102453] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:10:05.102532] Wet mask shape: torch.Size([352, 270, 180])
2025-09-06 23:10:06,253 - INFO - Training Epoch: [0]  [ 13/318]  eta: 3:06:26  lr: 0.000200  loss: 2.5273 (4.6281)  time: 3.808(3.808)  data: 0.013(0.013)  max cpu mem: 4314  max gpu mem: 136476
[23:10:08.440145] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:10:08.440220] Wet mask shape: torch.Size([352, 270, 180])
[23:10:09.034807] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:10:09.042625] Wet mask shape: torch.Size([352, 270, 180])
[23:10:09.235382] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:10:09.235508] Wet mask shape: torch.Size([352, 270, 180])
[23:10:09.586391] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:10:09.586472] Wet mask shape: torch.Size([352, 270, 180])
2025-09-06 23:10:10,702 - INFO - Training Epoch: [0]  [ 14/318]  eta: 2:54:56  lr: 0.000200  loss: 2.5273 (4.3713)  time: 4.448(4.448)  data: 0.005(0.005)  max cpu mem: 4314  max gpu mem: 136476
[23:10:12.783453] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:10:12.783533] Wet mask shape: torch.Size([352, 270, 180])
[23:10:13.213335] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:10:13.213406] Wet mask shape: torch.Size([352, 270, 180])
[23:10:13.588907] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:10:13.588981] Wet mask shape: torch.Size([352, 270, 180])
[23:10:13.977270] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:10:13.977354] Wet mask shape: torch.Size([352, 270, 180])
2025-09-06 23:10:15,104 - INFO - Training Epoch: [0]  [ 15/318]  eta: 2:44:51  lr: 0.000200  loss: 1.3150 (4.1404)  time: 4.400(4.400)  data: 0.000(0.000)  max cpu mem: 4314  max gpu mem: 136476
[23:14:43.031508] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:14:43.052631] Wet mask shape: torch.Size([352, 270, 180])
[23:14:43.416439] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:14:43.416515] Wet mask shape: torch.Size([352, 270, 180])
[23:14:43.826180] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:14:43.826253] Wet mask shape: torch.Size([352, 270, 180])
[23:14:44.122310] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:14:44.122381] Wet mask shape: torch.Size([352, 270, 180])
2025-09-06 23:14:45,261 - INFO - Training Epoch: [0]  [ 16/318]  eta: 3:54:38  lr: 0.000200  loss: 1.3150 (3.9338)  time: 270.157(270.157)  data: 266.756(266.756)  max cpu mem: 4314  max gpu mem: 136476
[23:14:46.796486] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:14:46.796563] Wet mask shape: torch.Size([352, 270, 180])
[23:14:47.129347] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:14:47.129418] Wet mask shape: torch.Size([352, 270, 180])
[23:14:47.525271] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:14:47.525359] Wet mask shape: torch.Size([352, 270, 180])
[23:14:47.829133] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:14:47.829201] Wet mask shape: torch.Size([352, 270, 180])
2025-09-06 23:14:48,950 - INFO - Training Epoch: [0]  [ 17/318]  eta: 3:41:53  lr: 0.000200  loss: 1.1733 (3.7475)  time: 3.679(3.679)  data: 0.000(0.000)  max cpu mem: 4314  max gpu mem: 136476
[23:14:51.315295] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:14:51.315381] Wet mask shape: torch.Size([352, 270, 180])
[23:14:51.772660] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:14:51.772736] Wet mask shape: torch.Size([352, 270, 180])
[23:14:51.996112] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:14:51.996182] Wet mask shape: torch.Size([352, 270, 180])
[23:14:52.469151] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:14:52.469223] Wet mask shape: torch.Size([352, 270, 180])
2025-09-06 23:14:53,615 - INFO - Training Epoch: [0]  [ 18/318]  eta: 3:30:44  lr: 0.000200  loss: 1.1733 (3.5760)  time: 4.656(4.656)  data: 0.000(0.000)  max cpu mem: 4314  max gpu mem: 136476
[23:14:55.078622] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:14:55.078692] Wet mask shape: torch.Size([352, 270, 180])
[23:14:55.432539] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:14:55.432608] Wet mask shape: torch.Size([352, 270, 180])
[23:14:55.767388] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:14:55.767461] Wet mask shape: torch.Size([352, 270, 180])
[23:14:56.036579] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:14:56.036652] Wet mask shape: torch.Size([352, 270, 180])
2025-09-06 23:14:57,179 - INFO - Training Epoch: [0]  [ 19/318]  eta: 3:20:25  lr: 0.000200  loss: 1.1197 (3.4304)  time: 3.560(3.560)  data: 0.000(0.000)  max cpu mem: 4314  max gpu mem: 136476
[23:15:34.148524] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:15:34.148737] Wet mask shape: torch.Size([352, 270, 180])
[23:15:34.490026] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:15:34.490119] Wet mask shape: torch.Size([352, 270, 180])
[23:15:34.836522] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:15:34.836595] Wet mask shape: torch.Size([352, 270, 180])
[23:15:35.202940] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:15:35.203064] Wet mask shape: torch.Size([352, 270, 180])
2025-09-06 23:15:36,333 - INFO - Training Epoch: [0]  [ 20/318]  eta: 3:19:30  lr: 0.000200  loss: 1.0800 (3.2906)  time: 39.151(39.151)  data: 0.000(0.000)  max cpu mem: 4314  max gpu mem: 136476
[23:16:27.794665] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:16:27.794766] Wet mask shape: torch.Size([352, 270, 180])
[23:16:27.931869] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:16:27.931929] Wet mask shape: torch.Size([352, 270, 180])
[23:16:28.070780] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:16:28.070836] Wet mask shape: torch.Size([352, 270, 180])
[23:16:28.207554] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:16:28.207608] Wet mask shape: torch.Size([352, 270, 180])
2025-09-06 23:16:29,316 - INFO - Training Epoch: [0]  [ 21/318]  eta: 3:21:42  lr: 0.000200  loss: 1.0569 (3.1636)  time: 52.981(52.981)  data: 0.008(0.008)  max cpu mem: 4314  max gpu mem: 136476
[23:16:29.668204] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:16:29.668263] Wet mask shape: torch.Size([352, 270, 180])
[23:16:29.805303] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:16:29.805363] Wet mask shape: torch.Size([352, 270, 180])
[23:16:29.942977] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:16:30.606206] Wet mask shape: torch.Size([352, 270, 180])
[23:16:30.737133] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:16:30.737185] Wet mask shape: torch.Size([352, 270, 180])
2025-09-06 23:16:31,848 - INFO - Training Epoch: [0]  [ 22/318]  eta: 3:12:50  lr: 0.000200  loss: 0.9492 (3.0526)  time: 2.531(2.531)  data: 0.000(0.000)  max cpu mem: 4314  max gpu mem: 136476
[23:16:32.203158] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:16:32.203221] Wet mask shape: torch.Size([352, 270, 180])
[23:16:32.340816] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:16:32.603523] Wet mask shape: torch.Size([352, 270, 180])
[23:16:32.738549] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:16:32.738598] Wet mask shape: torch.Size([352, 270, 180])
[23:16:32.876459] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:16:32.876515] Wet mask shape: torch.Size([352, 270, 180])
2025-09-06 23:16:33,983 - INFO - Training Epoch: [0]  [ 23/318]  eta: 3:04:37  lr: 0.000200  loss: 0.7758 (2.9467)  time: 2.134(2.134)  data: 0.000(0.000)  max cpu mem: 4314  max gpu mem: 136476
[23:16:34.331608] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:16:34.331667] Wet mask shape: torch.Size([352, 270, 180])
[23:16:34.469749] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:16:34.469799] Wet mask shape: torch.Size([352, 270, 180])
[23:16:34.607455] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:16:34.607519] Wet mask shape: torch.Size([352, 270, 180])
[23:16:34.746455] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:16:34.746519] Wet mask shape: torch.Size([352, 270, 180])
2025-09-06 23:16:35,855 - INFO - Training Epoch: [0]  [ 24/318]  eta: 2:56:59  lr: 0.000200  loss: 0.6769 (2.8500)  time: 1.871(1.871)  data: 0.000(0.000)  max cpu mem: 4314  max gpu mem: 136476
[23:17:55.336834] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:17:55.342982] Wet mask shape: torch.Size([352, 270, 180])
[23:17:55.707733] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:17:55.707807] Wet mask shape: torch.Size([352, 270, 180])
[23:17:56.179658] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:17:56.179742] Wet mask shape: torch.Size([352, 270, 180])
[23:17:56.534118] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:17:56.534195] Wet mask shape: torch.Size([352, 270, 180])
2025-09-06 23:17:57,700 - INFO - Training Epoch: [0]  [ 25/318]  eta: 3:04:59  lr: 0.000200  loss: 0.6651 (2.7628)  time: 81.843(81.843)  data: 0.000(0.000)  max cpu mem: 4314  max gpu mem: 136476
[23:17:58.957957] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:17:58.958033] Wet mask shape: torch.Size([352, 270, 180])
[23:17:59.265800] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:17:59.265875] Wet mask shape: torch.Size([352, 270, 180])
[23:17:59.581953] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:17:59.582029] Wet mask shape: torch.Size([352, 270, 180])
[23:18:00.077664] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:18:00.077738] Wet mask shape: torch.Size([352, 270, 180])
2025-09-06 23:18:01,236 - INFO - Training Epoch: [0]  [ 26/318]  eta: 2:58:09  lr: 0.000200  loss: 0.6285 (2.6805)  time: 3.535(3.535)  data: 0.000(0.000)  max cpu mem: 4314  max gpu mem: 136476
[23:18:02.621550] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:18:02.621656] Wet mask shape: torch.Size([352, 270, 180])
[23:18:02.899577] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:18:02.899667] Wet mask shape: torch.Size([352, 270, 180])
[23:18:03.199318] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:18:03.199395] Wet mask shape: torch.Size([352, 270, 180])
[23:18:03.419713] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:18:03.419797] Wet mask shape: torch.Size([352, 270, 180])
2025-09-06 23:18:04,536 - INFO - Training Epoch: [0]  [ 27/318]  eta: 2:51:46  lr: 0.000200  loss: 0.6114 (2.6024)  time: 3.299(3.299)  data: 0.001(0.001)  max cpu mem: 4314  max gpu mem: 136476
[23:18:06.451473] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:18:06.451567] Wet mask shape: torch.Size([352, 270, 180])
[23:18:06.642280] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:18:06.642364] Wet mask shape: torch.Size([352, 270, 180])
[23:18:06.921837] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:18:06.921916] Wet mask shape: torch.Size([352, 270, 180])
[23:18:07.179544] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:18:07.179619] Wet mask shape: torch.Size([352, 270, 180])
2025-09-06 23:18:08,294 - INFO - Training Epoch: [0]  [ 28/318]  eta: 2:45:54  lr: 0.000200  loss: 0.5832 (2.5315)  time: 3.747(3.747)  data: 0.000(0.000)  max cpu mem: 4314  max gpu mem: 136476
[23:18:09.532609] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:18:09.532691] Wet mask shape: torch.Size([352, 270, 180])
[23:18:09.854704] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:18:09.854792] Wet mask shape: torch.Size([352, 270, 180])
[23:18:10.197480] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:18:10.197559] Wet mask shape: torch.Size([352, 270, 180])
[23:18:10.465524] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:18:10.465602] Wet mask shape: torch.Size([352, 270, 180])
2025-09-06 23:18:11,606 - INFO - Training Epoch: [0]  [ 29/318]  eta: 2:40:21  lr: 0.000200  loss: 0.5810 (2.4616)  time: 3.310(3.310)  data: 0.000(0.000)  max cpu mem: 4314  max gpu mem: 136476
[23:18:13.617750] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:18:13.617843] Wet mask shape: torch.Size([352, 270, 180])
[23:18:13.981964] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:18:13.982070] Wet mask shape: torch.Size([352, 270, 180])
[23:18:14.190677] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:18:14.190752] Wet mask shape: torch.Size([352, 270, 180])
[23:18:14.520141] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:18:14.520215] Wet mask shape: torch.Size([352, 270, 180])
2025-09-06 23:18:15,656 - INFO - Training Epoch: [0]  [ 30/318]  eta: 2:35:16  lr: 0.000200  loss: 0.5458 (2.3994)  time: 4.049(4.049)  data: 0.000(0.000)  max cpu mem: 4314  max gpu mem: 136476
[23:18:17.341758] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:18:17.341870] Wet mask shape: torch.Size([352, 270, 180])
[23:18:17.692654] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:18:17.692737] Wet mask shape: torch.Size([352, 270, 180])
[23:18:18.009813] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:18:18.009900] Wet mask shape: torch.Size([352, 270, 180])
[23:18:18.389344] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:18:18.389424] Wet mask shape: torch.Size([352, 270, 180])
2025-09-06 23:18:19,522 - INFO - Training Epoch: [0]  [ 31/318]  eta: 2:30:28  lr: 0.000200  loss: 0.5458 (2.3449)  time: 3.863(3.863)  data: 0.000(0.000)  max cpu mem: 4314  max gpu mem: 136476
[23:21:47.540961] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:21:47.544975] Wet mask shape: torch.Size([352, 270, 180])
[23:21:47.713829] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:21:47.713922] Wet mask shape: torch.Size([352, 270, 180])
[23:21:48.004757] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:21:48.004839] Wet mask shape: torch.Size([352, 270, 180])
[23:21:48.157867] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:21:48.158718] Wet mask shape: torch.Size([352, 270, 180])
2025-09-06 23:21:49,271 - INFO - Training Epoch: [0]  [ 32/318]  eta: 2:55:42  lr: 0.000200  loss: 0.5458 (2.2925)  time: 209.742(209.742)  data: 207.166(207.166)  max cpu mem: 4314  max gpu mem: 136476
[23:21:50.528418] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:21:50.528502] Wet mask shape: torch.Size([352, 270, 180])
[23:21:50.833029] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:21:50.833106] Wet mask shape: torch.Size([352, 270, 180])
[23:21:51.165841] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:21:51.165922] Wet mask shape: torch.Size([352, 270, 180])
[23:21:51.485693] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:21:51.485765] Wet mask shape: torch.Size([352, 270, 180])
2025-09-06 23:21:52,634 - INFO - Training Epoch: [0]  [ 33/318]  eta: 2:50:24  lr: 0.000200  loss: 0.5407 (2.2386)  time: 3.362(3.362)  data: 0.014(0.014)  max cpu mem: 4314  max gpu mem: 136476
[23:21:53.669125] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:21:53.669212] Wet mask shape: torch.Size([352, 270, 180])
[23:21:54.055033] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:21:54.055128] Wet mask shape: torch.Size([352, 270, 180])
[23:21:54.305702] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:21:54.305794] Wet mask shape: torch.Size([352, 270, 180])
[23:21:54.713853] Model output shape in apply_corrections: torch.Size([2, 352, 270, 180])
[23:21:54.713971] Wet mask shape: torch.Size([352, 270, 180])
2025-09-06 23:21:55,855 - INFO - Training Epoch: [0]  [ 34/318]  eta: 2:45:23  lr: 0.000200  loss: 0.5308 (2.1859)  time: 3.220(3.220)  data: 0.000(0.000)  max cpu mem: 4314  max gpu mem: 136476
