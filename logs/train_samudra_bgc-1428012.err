[2025-09-07 11:28:48,997] torch.distributed.run: [WARNING] 
[2025-09-07 11:28:48,997] torch.distributed.run: [WARNING] *****************************************
[2025-09-07 11:28:48,997] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2025-09-07 11:28:48,997] torch.distributed.run: [WARNING] *****************************************
[rank1]:[E ProcessGroupNCCL.cpp:523] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=105573, OpType=BROADCAST, NumelIn=99906, NumelOut=99906, Timeout(ms)=600000) ran for 600669 milliseconds before timing out.
[rank1]:[E ProcessGroupNCCL.cpp:537] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank1]:[E ProcessGroupNCCL.cpp:543] To avoid data inconsistency, we are taking the entire process down.
[rank1]:[E ProcessGroupNCCL.cpp:1182] [Rank 1] NCCL watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=105573, OpType=BROADCAST, NumelIn=99906, NumelOut=99906, Timeout(ms)=600000) ran for 600669 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:525 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x15031ffbdd87 in /home/mk0964/.conda/envs/samudra/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1e6 (0x1503211656e6 in /home/mk0964/.conda/envs/samudra/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::workCleanupLoop() + 0x19d (0x150321168c3d in /home/mk0964/.conda/envs/samudra/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x119 (0x150321169839 in /home/mk0964/.conda/envs/samudra/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdbbf4 (0x15036ae89bf4 in /home/mk0964/.conda/envs/samudra/bin/../lib/libstdc++.so.6)
frame #5: <unknown function> + 0x8a19a (0x15036c88a19a in /lib64/libc.so.6)
frame #6: <unknown function> + 0x10f240 (0x15036c90f240 in /lib64/libc.so.6)

[2025-09-07 14:19:05,047] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 782046 closing signal SIGTERM
[2025-09-07 14:19:05,048] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 782048 closing signal SIGTERM
[2025-09-07 14:19:05,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 782049 closing signal SIGTERM
[2025-09-07 14:19:05,063] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: -6) local_rank: 1 (pid: 782047) of binary: /home/mk0964/.conda/envs/samudra/bin/python
Traceback (most recent call last):
  File "/home/mk0964/.conda/envs/samudra/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/mk0964/.conda/envs/samudra/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/home/mk0964/.conda/envs/samudra/lib/python3.10/site-packages/torch/distributed/run.py", line 812, in main
    run(args)
  File "/home/mk0964/.conda/envs/samudra/lib/python3.10/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/home/mk0964/.conda/envs/samudra/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/mk0964/.conda/envs/samudra/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
=======================================================
src/train.py FAILED
-------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
-------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-09-07_14:19:05
  host      : tiger-i04g1
  rank      : 1 (local_rank: 1)
  exitcode  : -6 (pid: 782047)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 782047
=======================================================
